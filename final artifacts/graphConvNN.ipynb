{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse, time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import os, sys\n",
    "from dgl.data import register_data_args, load_data\n",
    "import pandas as pd\n",
    "from gcn import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def _encode_onehot(labels):\n",
    "    classes = list(sorted(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def _sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return mask\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CernDataset class is built based on the strucurue of coreDataset </b><br>\n",
    "find it here: https://github.com/dmlc/dgl/blob/master/python/dgl/data/citation_graph.py <br>\n",
    "The code will build a fully connected graph between layer. each node (layercluster) is fully connected to all nodes from previous layer. In other word, each node is a parent for each and every node in the next layer. \n",
    "<br><br>\n",
    "<b>train_size</b> defines the propotion of nodes selected for training. <br>\n",
    "<b>val_size</b> defines the propotion of nodes selected for Validation.<br>\n",
    "all left (1 - train_size - val_size) is allocated for Test.(<b>test_size</b><br>\n",
    "actual validation may be less than <b>val_size</b> as after creating the mask we remove the nodes allocated for train from it using these two mask commands: <br>\n",
    "xor_val_train=np.bitwise_xor(val_mask,train_mask) <br>\n",
    "_val_mask=np.bitwise_and(val_mask,xor_val_train)<br><br>\n",
    "That means, test_size may be greater than actual \"1 - train_size - val_size\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CernDataset(object):\n",
    "    def getListEventLayerTrackster(self, df, event, trackster,layer):\n",
    "        filtered_df = df.loc[(df['event'] == event) & (df['layer'] ==layer) & (df['trackster'] ==trackster)]\n",
    "        return filtered_df\n",
    "\n",
    "    def __init__(self, train_size=0.15, val_size=0.50):\n",
    "         self.store_oct27 = pd.read_hdf(\"singlepi_e100GeV_pu200_oct27.h5\")\n",
    "         #filter out some events - to minimize the processing time during development.\n",
    "         #self.store_oct27 = self.store_oct27[self.store_oct27['event'] < 5 ]\n",
    "         self.store_oct27['purity']=self.store_oct27['purity'].apply(lambda x: 0 if x <=1 else 1 )\n",
    "         self.df = self.store_oct27.drop(['eta','phi','layer','trckPhi','trckEn','trckEta','trckType'],1,inplace=False)\n",
    "         self.train_size = train_size\n",
    "         self.val_size = val_size\n",
    "         self._load()\n",
    "         \n",
    "    def _load(self):\n",
    "        \n",
    "        idx_features_labels =  self.df.drop(['purity','event','trackster'],1,inplace=False)\n",
    "        idx_features_labels = idx_features_labels.to_numpy()\n",
    "        features = sp.csr_matrix(idx_features_labels,dtype=np.float32)\n",
    "        labels = _encode_onehot(self.df[['purity']].iloc[:,0])\n",
    "        self.num_labels = labels.shape[1]\n",
    "        \n",
    "        # build graph\n",
    "        edges_flatted =[]\n",
    "        for idx, row in self.store_oct27.iterrows():\n",
    "            prev_layer = self.getListEventLayerTrackster(self.store_oct27, row['event'],row['trackster'],row['layer']-1)\n",
    "            for jdx,row in prev_layer.iterrows():\n",
    "                edges_flatted.append(jdx)\n",
    "                edges_flatted.append(idx)\n",
    "                \n",
    "        edges= np.array(edges_flatted).reshape(len(edges_flatted) // 2,2)\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]),\n",
    "                             (edges[:, 0], edges[:, 1])),\n",
    "                            shape=(labels.shape[0], labels.shape[0]),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "        self.graph = nx.from_scipy_sparse_matrix(adj, create_using=nx.DiGraph())\n",
    "\n",
    "        features = _normalize(features)\n",
    "        self.features = np.array(features.todense())\n",
    "        self.labels = np.where(labels)[1]\n",
    "        #test_size = int(labels.shape[0] * 0.15)\n",
    "        train_size = int(labels.shape[0] * self.train_size)\n",
    "        val_size = int(labels.shape[0] * self.val_size)\n",
    "        \n",
    "        train_mask= np.zeros(labels.shape[0], dtype=bool)\n",
    "        train_mask[:train_size] = 1\n",
    "        np.random.shuffle(train_mask)\n",
    "        self.train_mask = train_mask\n",
    "        \n",
    "        val_mask= np.zeros(labels.shape[0], dtype=bool)\n",
    "        val_mask[:val_size] = 1\n",
    "        np.random.shuffle(val_mask)\n",
    "        xor_val_train=np.bitwise_xor(val_mask,train_mask)\n",
    "        _val_mask=np.bitwise_and(val_mask,xor_val_train)\n",
    "        \n",
    "        self.val_mask = _val_mask.tolist()\n",
    "        #all layercluster not chosen for training or validation will be added to the test \n",
    "        test_mask = np.bitwise_or(self.val_mask, self.train_mask)\n",
    "        _test_mask = np.invert(test_mask)\n",
    "        self.test_mask = _test_mask.tolist()\n",
    "        #self.train_mask = _sample_mask(range(1500), labels.shape[0])\n",
    "        #self.val_mask = _sample_mask(range(2000, 5000), labels.shape[0])\n",
    "        #self.test_mask = _sample_mask(range(5000, 8000), labels.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        g = DGLGraph(self.graph)\n",
    "        g.ndata['train_mask'] = self.train_mask\n",
    "        g.ndata['val_mask'] = self.val_mask\n",
    "        g.ndata['test_mask'] = self.test_mask\n",
    "        g.ndata['label'] = self.labels\n",
    "        g.ndata['feat'] = self.features\n",
    "        return g\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defind the main model building function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphCovNN(args):\n",
    "    # load and preprocess dataset\n",
    "    data = args.dataset\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = torch.BoolTensor(data.train_mask)\n",
    "        val_mask = torch.BoolTensor(data.val_mask)\n",
    "        test_mask = torch.BoolTensor(data.test_mask)\n",
    "    else:\n",
    "        train_mask = torch.ByteTensor(data.train_mask)\n",
    "        val_mask = torch.ByteTensor(data.val_mask)\n",
    "        test_mask = torch.ByteTensor(data.test_mask)\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = data.num_labels\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.int().sum().item(),\n",
    "              val_mask.int().sum().item(),\n",
    "              test_mask.int().sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        g.remove_edges_from(nx.selfloop_edges(g))\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        acc = evaluate(model, features, labels, val_mask)\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} |\"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                             n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    print()\n",
    "    acc = evaluate(model, features, labels, test_mask)\n",
    "    print(\"Test accuracy {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Edges 514910\n",
      "      #Classes 2\n",
      "      #Train samples 4841\n",
      "      #Val samples 13710\n",
      "      #Test samples 13723\n",
      "Epoch 00000 | Time(s) nan | Loss 0.6637 |ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.6272 |ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.5939 |ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.1765 | Loss 0.5663 |ETputs(KTEPS) 2916.88\n",
      "Epoch 00004 | Time(s) 0.1740 | Loss 0.5385 |ETputs(KTEPS) 2958.68\n",
      "Epoch 00005 | Time(s) 0.1715 | Loss 0.5230 |ETputs(KTEPS) 3001.60\n",
      "Epoch 00006 | Time(s) 0.1769 | Loss 0.5179 |ETputs(KTEPS) 2910.43\n",
      "Epoch 00007 | Time(s) 0.1741 | Loss 0.5247 |ETputs(KTEPS) 2958.36\n",
      "Epoch 00008 | Time(s) 0.1727 | Loss 0.5320 |ETputs(KTEPS) 2981.12\n",
      "Epoch 00009 | Time(s) 0.1725 | Loss 0.5280 |ETputs(KTEPS) 2985.26\n",
      "Epoch 00010 | Time(s) 0.1711 | Loss 0.5275 |ETputs(KTEPS) 3010.14\n",
      "Epoch 00011 | Time(s) 0.1697 | Loss 0.5232 |ETputs(KTEPS) 3034.75\n",
      "Epoch 00012 | Time(s) 0.1717 | Loss 0.5210 |ETputs(KTEPS) 2998.82\n",
      "Epoch 00013 | Time(s) 0.1700 | Loss 0.5156 |ETputs(KTEPS) 3028.66\n",
      "Epoch 00014 | Time(s) 0.1689 | Loss 0.5076 |ETputs(KTEPS) 3047.97\n",
      "\n",
      "Test accuracy 80.30%\n"
     ]
    }
   ],
   "source": [
    "#configularble paramters \n",
    "args=dummy()\n",
    "args.dropout=0.3 #dropout probability\n",
    "args.gpu=-1\n",
    "args.lr=0.01 #learning rate\n",
    "args.n_epochs=15\n",
    "args.n_hidden=16 #number of hidden gcn units\n",
    "args.n_layers=3\n",
    "args.self_loop=False #graph self-loop \n",
    "args.weight_decay=0.0005\n",
    "args.dataset = CernDataset()\n",
    "graphCovNN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Edges 514910\n",
      "      #Classes 2\n",
      "      #Train samples 20978\n",
      "      #Val samples 2809\n",
      "      #Test samples 8487\n",
      "Epoch 00000 | Time(s) nan | Loss 0.7216 |ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.9240 |ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.6477 |ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.2718 | Loss 0.5574 |ETputs(KTEPS) 1894.60\n",
      "Epoch 00004 | Time(s) 0.2725 | Loss 0.5102 |ETputs(KTEPS) 1889.37\n",
      "Epoch 00005 | Time(s) 0.2695 | Loss 0.4951 |ETputs(KTEPS) 1910.39\n",
      "Epoch 00006 | Time(s) 0.2804 | Loss 0.5118 |ETputs(KTEPS) 1836.06\n",
      "Epoch 00007 | Time(s) 0.2776 | Loss 0.5008 |ETputs(KTEPS) 1854.77\n",
      "Epoch 00008 | Time(s) 0.2746 | Loss 0.4964 |ETputs(KTEPS) 1874.79\n",
      "Epoch 00009 | Time(s) 0.2727 | Loss 0.4953 |ETputs(KTEPS) 1887.87\n",
      "\n",
      "Test accuracy 80.45%\n"
     ]
    }
   ],
   "source": [
    "# try with different setup.\n",
    "args1=dummy()\n",
    "args1.dropout=0.5 #dropout probability\n",
    "args1.gpu=-1\n",
    "args1.lr=0.1 #learning rate\n",
    "args1.n_epochs=10\n",
    "args1.n_hidden=32 #number of hidden gcn units\n",
    "args1.n_layers=3\n",
    "args1.self_loop=False #graph self-loop \n",
    "args1.weight_decay=0.0005\n",
    "args1.dataset = CernDataset(0.65, 0.25)\n",
    "\n",
    "graphCovNN(args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Edges 514910\n",
      "      #Classes 2\n",
      "      #Train samples 22591\n",
      "      #Val samples 1756\n",
      "      #Test samples 7927\n",
      "Epoch 00000 | Time(s) nan | Loss 0.6956 |ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.8096 |ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.9065 |ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.3616 | Loss 0.5571 |ETputs(KTEPS) 1424.10\n",
      "Epoch 00004 | Time(s) 0.3509 | Loss 0.4992 |ETputs(KTEPS) 1467.49\n",
      "Epoch 00005 | Time(s) 0.3312 | Loss 0.5304 |ETputs(KTEPS) 1554.83\n",
      "Epoch 00006 | Time(s) 0.3162 | Loss 0.5058 |ETputs(KTEPS) 1628.19\n",
      "Epoch 00007 | Time(s) 0.3162 | Loss 0.4986 |ETputs(KTEPS) 1628.26\n",
      "Epoch 00008 | Time(s) 0.3162 | Loss 0.4969 |ETputs(KTEPS) 1628.30\n",
      "Epoch 00009 | Time(s) 0.3172 | Loss 0.4953 |ETputs(KTEPS) 1623.19\n",
      "Epoch 00010 | Time(s) 0.3147 | Loss 0.4960 |ETputs(KTEPS) 1636.40\n",
      "Epoch 00011 | Time(s) 0.3190 | Loss 0.4975 |ETputs(KTEPS) 1613.90\n",
      "Epoch 00012 | Time(s) 0.3148 | Loss 0.4963 |ETputs(KTEPS) 1635.56\n",
      "Epoch 00013 | Time(s) 0.3229 | Loss 0.4956 |ETputs(KTEPS) 1594.51\n",
      "Epoch 00014 | Time(s) 0.3235 | Loss 0.4946 |ETputs(KTEPS) 1591.53\n",
      "Epoch 00015 | Time(s) 0.3215 | Loss 0.4969 |ETputs(KTEPS) 1601.71\n",
      "Epoch 00016 | Time(s) 0.3197 | Loss 0.4962 |ETputs(KTEPS) 1610.38\n",
      "Epoch 00017 | Time(s) 0.3217 | Loss 0.4948 |ETputs(KTEPS) 1600.43\n",
      "Epoch 00018 | Time(s) 0.3216 | Loss 0.4942 |ETputs(KTEPS) 1601.07\n",
      "Epoch 00019 | Time(s) 0.3185 | Loss 0.4948 |ETputs(KTEPS) 1616.54\n",
      "\n",
      "Test accuracy 81.22%\n"
     ]
    }
   ],
   "source": [
    "# try with different setup.\n",
    "args2=dummy()\n",
    "args2.dropout=0.5 #dropout probability\n",
    "args2.gpu=-1\n",
    "args2.lr=0.1 #learning rate\n",
    "args2.n_epochs=20\n",
    "args2.n_hidden=32 #number of hidden gcn units\n",
    "args2.n_layers=3\n",
    "args2.self_loop=False #graph self-loop \n",
    "args2.weight_decay=0.0005\n",
    "args2.dataset = CernDataset(0.7, 0.18)\n",
    "\n",
    "graphCovNN(args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Edges 514910\n",
      "      #Classes 2\n",
      "      #Train samples 22591\n",
      "      #Val samples 2325\n",
      "      #Test samples 7358\n",
      "Epoch 00000 | Time(s) nan | Loss 0.6746 |ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.9232 |ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.9294 |ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.4402 | Loss 0.5855 |ETputs(KTEPS) 1169.68\n",
      "Epoch 00004 | Time(s) 0.4263 | Loss 0.5247 |ETputs(KTEPS) 1207.83\n",
      "Epoch 00005 | Time(s) 0.4215 | Loss 0.5019 |ETputs(KTEPS) 1221.56\n",
      "Epoch 00006 | Time(s) 0.4205 | Loss 0.4923 |ETputs(KTEPS) 1224.55\n",
      "Epoch 00007 | Time(s) 0.4184 | Loss 0.4911 |ETputs(KTEPS) 1230.71\n",
      "Epoch 00008 | Time(s) 0.4221 | Loss 0.4900 |ETputs(KTEPS) 1219.80\n",
      "Epoch 00009 | Time(s) 0.4134 | Loss 0.4889 |ETputs(KTEPS) 1245.55\n",
      "Epoch 00010 | Time(s) 0.4138 | Loss 0.4872 |ETputs(KTEPS) 1244.23\n",
      "Epoch 00011 | Time(s) 0.4136 | Loss 0.4879 |ETputs(KTEPS) 1244.87\n",
      "Epoch 00012 | Time(s) 0.4092 | Loss 0.4868 |ETputs(KTEPS) 1258.43\n",
      "Epoch 00013 | Time(s) 0.4084 | Loss 0.4855 |ETputs(KTEPS) 1260.87\n",
      "Epoch 00014 | Time(s) 0.4046 | Loss 0.4871 |ETputs(KTEPS) 1272.51\n",
      "Epoch 00015 | Time(s) 0.4009 | Loss 0.4872 |ETputs(KTEPS) 1284.25\n",
      "Epoch 00016 | Time(s) 0.3986 | Loss 0.4880 |ETputs(KTEPS) 1291.94\n",
      "Epoch 00017 | Time(s) 0.3983 | Loss 0.4868 |ETputs(KTEPS) 1292.71\n",
      "Epoch 00018 | Time(s) 0.3975 | Loss 0.4840 |ETputs(KTEPS) 1295.42\n",
      "Epoch 00019 | Time(s) 0.4007 | Loss 0.4820 |ETputs(KTEPS) 1285.17\n",
      "Epoch 00020 | Time(s) 0.3993 | Loss 0.4751 |ETputs(KTEPS) 1289.48\n",
      "Epoch 00021 | Time(s) 0.4008 | Loss 0.4851 |ETputs(KTEPS) 1284.73\n",
      "Epoch 00022 | Time(s) 0.4012 | Loss 0.4771 |ETputs(KTEPS) 1283.51\n",
      "Epoch 00023 | Time(s) 0.4038 | Loss 0.4939 |ETputs(KTEPS) 1275.01\n",
      "Epoch 00024 | Time(s) 0.4025 | Loss 0.5035 |ETputs(KTEPS) 1279.31\n",
      "Epoch 00025 | Time(s) 0.4048 | Loss 0.5039 |ETputs(KTEPS) 1271.98\n",
      "Epoch 00026 | Time(s) 0.4036 | Loss 0.5004 |ETputs(KTEPS) 1275.70\n",
      "Epoch 00027 | Time(s) 0.4053 | Loss 0.4881 |ETputs(KTEPS) 1270.38\n",
      "Epoch 00028 | Time(s) 0.4056 | Loss 0.4908 |ETputs(KTEPS) 1269.58\n",
      "Epoch 00029 | Time(s) 0.4049 | Loss 0.4882 |ETputs(KTEPS) 1271.71\n",
      "\n",
      "Test accuracy 80.47%\n"
     ]
    }
   ],
   "source": [
    "# try with different setup.\n",
    "#args2=dummy()\n",
    "args2.dropout=0.5 #dropout probability\n",
    "args2.gpu=-1\n",
    "args2.lr=0.1 #learning rate\n",
    "args2.n_epochs=30\n",
    "args2.n_hidden=32 #number of hidden gcn units\n",
    "args2.n_layers=4\n",
    "args2.self_loop=False #graph self-loop \n",
    "args2.weight_decay=0.0005\n",
    "#args2.dataset = CernDataset(0.70, 0.25)\n",
    "\n",
    "graphCovNN(args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
